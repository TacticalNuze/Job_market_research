import asyncio
import os
import json
from crawl4ai import AsyncWebCrawler
from groq import Groq
from jsonschema import validate, ValidationError

# --- Configuration ---
os.environ["GROQ_API_KEY"] = "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"
client = Groq(api_key=os.environ["GROQ_API_KEY"])

# URL de base (sans param√®tre de page)
BASE_URL = "https://www.rekrute.com/offres.html?st=d&keywordNew=1&jobLocation=RK&tagSearchKey=&keyword=data"
BASE_URL = BASE_URL.strip()

# Sch√©ma JSON pour validation
JOB_SCHEMA = {
    "type": "object",
    "properties": {
        "job_url": {"type": "string"},
        "titre": {"type": "string"},
        "companie": {"type": "string"},
        "description": {"type": "string"},
        "niveau_etudes": {"type": "string"},
        "niveau_experience": {"type": ["string", "null"]},
        "contrat": {"type": "string"},
        "region": {"type": "string"},
        "competences": {"type": "string"},
        "secteur": {"type": "string"},
        "salaire": {"type": ["integer", "string"]},
        "domaine": {"type": "string"},
        "extra": {"type": "string"},
        "via": {"type": "string"},
        "publication_date": {"type": "string", "format": "date"}
    },
    "required": ["job_url", "titre", "via", "publication_date"]
}


# V√©rifier l'existence du fichier et charger le sch√©ma JSON
#if not os.path.exists(SCHEMA_PATH):
 #   print(f"[‚ùå] Fichier {SCHEMA_PATH} introuvable. V√©rifiez le chemin ou utilisez un chemin absolu : D:\\Job_market_research-main\\Data_extraction\\Websites\\job_schema.json")
  #  exit(1)

#try:
 #   with open(SCHEMA_PATH, "r", encoding="utf-8") as f:
  #      JOB_SCHEMA = json.load(f)
   # print("[‚úÖ] Sch√©ma JSON charg√© avec succ√®s depuis", SCHEMA_PATH)
#except json.JSONDecodeError as e:
 #   print(f"[‚ùå] Erreur lors du d√©codage du fichier JSON {SCHEMA_PATH} : {e}")
  #  exit(1)


# Prompt pour Groq
PROMPT = """
Tu es un extracteur sp√©cialis√© en offres d'emploi.

√Ä partir de cette page Rekrute.com, extrait toutes les offres disponibles sous forme de tableau JSON avec la cl√© "offres". Chaque offre doit contenir les champs suivants :
- job_url (URL de l'offre, obligatoire)
- titre (titre de l'offre, obligatoire)
- companie (nom de l'entreprise)
- description (description de l'offre)
- niveau_etudes (niveau d'√©tudes requis)
- niveau_experience (niveau d'exp√©rience requis, peut √™tre null)
- contrat (type de contrat)
- region (r√©gion g√©ographique)
- competences (comp√©tences cl√©s)
- secteur (secteur d'activit√©)
- salaire (salaire, peut √™tre un entier ou une cha√Æne)
- domaine (domaine de l'offre)
- extra (informations suppl√©mentaires)
- via (source de l'offre, par exemple "Rekrute", obligatoire)
- publication_date (date de publication au format YYYY-MM-DD, obligatoire)

Si une information n'est pas trouv√©e, utilise "N/A" sauf pour les champs obligatoires (job_url, titre, via, publication_date), qui doivent √™tre pr√©sents. Format de sortie : uniquement un tableau JSON valide avec la cl√© "offres", sans texte suppl√©mentaire.
"""

async def scrape_and_extract_with_groq():
    """Scrape les pages de Rekrute et extrait les offres d'emploi avec Crawl4AI et Groq.

    Parcourt les pages pagin√©es, extrait le contenu HTML, et utilise Groq pour structurer les offres en JSON.

    Returns:
        str: Cha√Æne JSON contenant les offres extraites sous la cl√© "offres", ou None si aucune donn√©e n'est extraite.
    """
    all_offers = []
    page = 1

    while True:
        # Construire l'URL pagin√©e
        paginated_url = f"{BASE_URL}&page={page}"
        print(f"[INFO] Scraping de la page {page} : {paginated_url}")

        async with AsyncWebCrawler(verbose=True) as crawler:
            result = await crawler.arun(
                url=paginated_url,
                js_code="""
                const selectors = ['.offer', '.job-item', '[class*="job"]', '[class*="offer"]'];
                let offers = [];
                let attempt = 0;

                const waitForJobs = () => {
                    return new Promise((resolve) => {
                        const check = () => {
                            for (let selector of selectors) {
                                offers = document.querySelectorAll(selector);
                                if (offers.length > 0) {
                                    let offerHtml = '';
                                    offers.forEach(offer => {
                                        offerHtml += offer.outerHTML + '\n';
                                    });
                                    document.body.innerHTML = offerHtml;
                                    resolve('Offres trouv√©es avec ' + selector);
                                    return;
                                }
                            }
                            if (attempt < 20) {
                                attempt++;
                                setTimeout(check, 1000);
                            } else {
                                resolve(false);
                            }
                        };
                        check();
                    });
                };
                return waitForJobs();
                """
            )

            if not result.success:
                print(f"[‚ùå] √âchec lors du chargement de la page {page}.")
                break

            print("[‚úÖ] Page charg√©e avec succ√®s.")
            print("[DEBUG] Longueur du HTML nettoy√© :", len(result.cleaned_html))

            # V√©rifier si la page est vide ou indique la fin
            if len(result.cleaned_html) < 1000 or "aucune offre" in result.cleaned_html.lower():
                print(f"[INFO] Fin des pages d√©tect√©e √† la page {page}.")
                break

            # Diviser le HTML en chunks pour respecter la limite de tokens
            chunk_size = 5000
            html_chunks = [result.cleaned_html[i:i + chunk_size] for i in range(0, len(result.cleaned_html), chunk_size)]
            print(f"[INFO] Nombre de chunks cr√©√©s pour la page {page} : {len(html_chunks)}")

            # Extraire les donn√©es de chaque chunk
            page_offers = []
            for chunk_idx, chunk in enumerate(html_chunks):
                print(f"[INFO] Traitement du chunk {chunk_idx + 1}/{len(html_chunks)} (longueur : {len(chunk)} caract√®res)")
                try:
                    completion = client.chat.completions.create(
                        model="llama-3.3-70b-versatile",
                        response_format={"type": "json_object"},
                        messages=[
                            {"role": "system", "content": "Vous √™tes un assistant sp√©cialis√© dans l'extraction structur√©e d'informations."},
                            {"role": "user", "content": f"{PROMPT}\n\nVoici le HTML de la page emploi :\n\n{chunk}"}
                        ]
                    )
                    chunk_data = json.loads(completion.choices[0].message.content)
                    if "offres" in chunk_data:
                        page_offers.extend(chunk_data["offres"])
                    else:
                        print(f"[‚ö†Ô∏è] Aucun 'offres' trouv√© dans le chunk {chunk_idx + 1}")
                except Groq.APIStatusError as e:
                    print(f"[‚ùå] Erreur API Groq pour le chunk {chunk_idx + 1} : {e}")
                    continue
                except Exception as e:
                    print(f"[‚ùå] Erreur inattendue pour le chunk {chunk_idx + 1} : {e}")
                    continue

                if chunk_idx < len(html_chunks) - 1:
                    print("[INFO] Pause de 5 secondes pour √©viter la limite de tokens...")
                    await asyncio.sleep(5)

            all_offers.extend(page_offers)
            page += 1

        if not page_offers:
            print(f"[INFO] Aucune offre trouv√©e sur la page {page}, arr√™t de la pagination.")
            break

    if not all_offers:
        print("[‚ùå] Aucune donn√©e extraite apr√®s traitement de toutes les pages.")
        return None

    extracted_data = json.dumps({"offres": all_offers})
    print("[DEBUG] Donn√©es extraites brutes :", extracted_data)
    return extracted_data

async def main():
    """Ex√©cute le processus complet de scraping et d'extraction des offres d'emploi sur Rekrute.

    Coordonne le scraping des pages, l'extraction des donn√©es, leur validation, et leur sauvegarde.
    """
    print("[üîç] D√©marrage du scraping et extraction IA...\n")
    extracted_content = await scrape_and_extract_with_groq()

    if not extracted_content:
        print("\n‚ùå Aucune donn√©e extraite.")
        return

    try:
        data = json.loads(extracted_content)
        if "offres" not in data:
            print("\n‚ùå Les donn√©es extraites ne contiennent pas la cl√© 'offres'.")
            return
        offers = data["offres"]
    except json.JSONDecodeError as e:
        print(f"\n‚ùå Erreur lors du d√©codage JSON : {e}")
        return

    valid_offers = []
    for i, offer in enumerate(offers):
        try:
            validate(instance=offer, schema=JOB_SCHEMA)
            valid_offers.append(offer)
            print(f"[‚úÖ] Offre {i + 1} valid√©e avec succ√®s.")
        except ValidationError as e:
            print(f"[‚ö†Ô∏è] Offre {i + 1} non valide : {e.message}")

    if valid_offers:
        final_data = {"offres": valid_offers}
        print("\nüß† Donn√©es valid√©es :")
        print(json.dumps(final_data, indent=2))

        with open("offres_emploi_groq.json", "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

        print("\nüíæ Donn√©es sauvegard√©es dans 'offres_emploi_groq.json'")
    else:
        print("\n‚ùå Aucune offre valide √† sauvegarder.")

if __name__ == "__main__":
    asyncio.run(main())