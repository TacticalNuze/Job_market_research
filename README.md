# Job_market_research

Un pipeline complet pour l'extraction, l'analyse et la pr√©diction des tendances du march√© de l'emploi en Intelligence Artificielle (IA) et Big Data.

## Table des mati√®res

- [Aper√ßu](#aper√ßu)
- [Architecture du projet](#architecture-du-projet)
- [Fonctionnalit√©s](#fonctionnalit√©s)
- [Installation](#installation)
- [Configuration](#configuration)
- [Utilisation](#utilisation)
- [Exemples de r√©sultats](#exemples-de-r√©sultats)
- [Technologies utilis√©es](#technologies-utilis√©es)
- [Bonnes pratiques et conseils](#bonnes-pratiques-et-conseils)
- [Contribuer](#contribuer)
- [Feuille de route](#feuille-de-route)
- [FAQ](#faq)
- [Licence](#licence)

## Aper√ßu

Ce projet propose un pipeline automatis√© pour extraire des offres d'emploi en IA et Big Data depuis diverses sources web, nettoyer et pr√©traiter les donn√©es, analyser les tendances du march√© et effectuer des pr√©dictions sur l'√©volution des offres. L'objectif est d'aider les chercheurs d'emploi, recruteurs et analystes √† mieux comprendre le march√© de l'emploi dans ces domaines.

### Objectifs principaux

- **Centraliser** les offres d'emploi issues de multiples plateformes.
- **Analyser** les tendances du march√© (comp√©tences, localisation, √©volution temporelle).
- **Pr√©dire** l'√©volution du march√© gr√¢ce √† des mod√®les de machine learning.
- **Automatiser** la g√©n√©ration de rapports et de visualisations.

## Architecture du projet

```
üìÅ Project Root
‚îÇ
‚îú‚îÄ‚îÄ ai_models/                    # Models and scripts related to AI functionalities
‚îú‚îÄ‚îÄ celery_app/                  # Celery task definitions and configurations
‚îú‚îÄ‚îÄ data_extraction/            # Web scraping or data ingestion scripts
‚îú‚îÄ‚îÄ database/                   # Database-related scripts (e.g., schemas, initialization)
‚îú‚îÄ‚îÄ docker-entrypoint-initdb.d/ # SQL or scripts to initialize the PostgreSQL container
‚îú‚îÄ‚îÄ documents/                  # Reference or documentation files
‚îú‚îÄ‚îÄ enrechissement_process/     # Data enrichment processes and scripts
‚îú‚îÄ‚îÄ output/                     # Output files or temporary results
‚îú‚îÄ‚îÄ postgres/                   # PostgreSQL-related configurations
‚îú‚îÄ‚îÄ skillner/                   # Named Entity Recognition for skill extraction
‚îú‚îÄ‚îÄ spark_pipeline/             # Spark pipelines for data transformation
‚îú‚îÄ‚îÄ superset/                   # Apache Superset configuration and assets
‚îú‚îÄ‚îÄ traitement/                 # Data cleaning or transformation scripts
‚îÇ
‚îú‚îÄ‚îÄ dockercompose.dev.yaml      # Docker Compose file for development environment
‚îú‚îÄ‚îÄ dockercompose.prod.yaml     # Docker Compose file for production environment
‚îú‚îÄ‚îÄ Dockerfile                  # Dockerfile for containerizing the app
‚îú‚îÄ‚îÄ prometheus.yml              # Prometheus monitoring configuration
‚îú‚îÄ‚îÄ pyproject.toml              # Python project metadata and dependencies
‚îú‚îÄ‚îÄ README.md                   # Project documentation
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies list
‚îú‚îÄ‚îÄ skill_db_relax_20.json      # JSON database of skills
‚îú‚îÄ‚îÄ uv.lock                     # Lockfile for dependency versions (used with uv)

```


## Fonctionnalit√©s

- **Scraping automatis√©** : Extraction des offres d'emploi depuis plusieurs sites sp√©cialis√©s (Indeed, LinkedIn, etc.).
- **Nettoyage et pr√©traitement** : Standardisation, suppression des doublons, gestion des valeurs manquantes, normalisation des intitul√©s de postes et des comp√©tences.
- **Analyse des tendances** : Statistiques descriptives, visualisations (√©volution temporelle, r√©partition g√©ographique, comp√©tences demand√©es, salaires, etc.).
- **Pr√©diction** : Mod√®les de machine learning pour anticiper les tendances du march√© (r√©gression, s√©ries temporelles).
- **Rapports automatis√©s** : G√©n√©ration de graphiques et de rapports dans le dossier `output/`.
- **Extensibilit√©** : Ajout facile de nouvelles sources ou de nouveaux modules d'analyse.

## Installation

1. **Clonez le d√©p√¥t :**
    ```bash
    git clone https://github.com/TacticalNuze/Job_market_research.git
    cd Job_market_research
    ```
2. **Installez les d√©pendances :**
    ```bash
    pip install -r requirements.txt
    ```
3. **(Optionnel) Cr√©ez un environnement virtuel :**
    ```bash
    python -m venv venv
    source venv/bin/activate  # ou venv\Scripts\activate sous Windows
    ```

## Configuration

Avant d'ex√©cuter le pipeline, configurez les sources de donn√©es et les param√®tres dans le fichier `config.yaml`. Exemple de configuration¬†:

```yaml
sources:
  - name: Indeed
    url: "https://www.indeed.com/jobs?q=big+data"
  - name: LinkedIn
    url: "https://www.linkedin.com/jobs/search/?keywords=IA"
params:
  max_pages: 10
  output_dir: "output/"
  language: "fr"
  country: "FR"
```

**Conseil :** Vous pouvez ajouter d'autres sources ou ajuster les param√®tres selon vos besoins.

## Utilisation

1. **V√©rifiez la configuration dans `config.yaml`.**
2. **Lancez le pipeline :**
    ```bash
    python main.py
    ```
3. **Consultez les r√©sultats :** Les donn√©es trait√©es, visualisations et rapports sont disponibles dans le dossier `output/`.

### Ex√©cution pas √† pas

- Pour ex√©cuter uniquement le scraping :
    ```bash
    python src/scraping/scrape.py
    ```
- Pour lancer l'analyse ou la pr√©diction s√©par√©ment, ex√©cutez les scripts correspondants dans `src/analysis/` ou `src/prediction/`.

## Exemples de r√©sultats

- Graphiques de l'√©volution des offres d'emploi par mois
- Cartes de r√©partition g√©ographique des offres
- Nuages de mots des comp√©tences les plus demand√©es
- Pr√©dictions sur l'√©volution du volume d'offres
- Tableaux de synth√®se des salaires et des types de contrats

*(Voir le dossier `output/` pour des exemples concrets)*

## Technologies utilis√©es

- **Python** (3.8+)
- **Pandas** (traitement des donn√©es)
- **BeautifulSoup / Scrapy** (scraping web)
- **Scikit-learn** (mod√©lisation pr√©dictive)
- **Matplotlib / Seaborn** (visualisation)
- **PyYAML** (gestion de la configuration)
- **Jupyter Notebook** (exploration interactive, prototypage)

## Bonnes pratiques et conseils

- Respectez les conditions d'utilisation des sites web lors du scraping.
- Mettez √† jour r√©guli√®rement les d√©pendances (`pip install --upgrade -r requirements.txt`).
- Sauvegardez vos donn√©es brutes avant tout traitement.
- Documentez vos modifications et tests dans des notebooks ou des fichiers markdown.
- Respectez le format du fichier "Job_schema.json" lors du scraping des donn√©es.
## Contribuer

Les contributions sont les bienvenues ! Pour proposer une am√©lioration ou corriger un bug¬†:

1. Forkez le projet
2. Cr√©ez une branche (`git checkout -b feature/ma-feature`)
3. Commitez vos modifications
4. Ouvrez une Pull Request

N'h√©sitez pas √† ouvrir une issue pour toute question ou suggestion.

## Feuille de route

- [ ] Ajout de nouvelles sources d'offres d'emploi (Monster, Glassdoor, etc.)
- [ ] Am√©lioration des mod√®les pr√©dictifs (deep learning, s√©ries temporelles avanc√©es)
- [ ] Tableau de bord interactif (Streamlit, Dash)
- [ ] Internationalisation (support multilingue)
- [ ] Int√©gration continue et tests automatis√©s

## FAQ

**Q : Puis-je ajouter mes propres sources de donn√©es ?**  
R : Oui, il suffit d'ajouter une entr√©e dans `config.yaml` et d'impl√©menter un module de scraping adapt√© si besoin.

**Q : Le projet fonctionne-t-il sous Windows, Linux et MacOS ?**  
R : Oui, le pipeline est compatible avec les principaux syst√®mes d'exploitation.

**Q : Comment signaler un bug ?**  
R : Ouvrez une issue sur GitHub avec une description d√©taill√©e.

## Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus d'informations.
