import json
import logging
from datetime import datetime
import time
import os

from utils__init__ import (
    read_and_normalize_all_offers,
    save_to_minio,
)

from init_groq import (
    call_groq,
    validate_profile_fields,
    deduplicate_profiles,
    test_groq_connection,
)

# Configuration des logs
logging.basicConfig(
    level=logging.INFO, 
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f"groq_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    ]
)
logger = logging.getLogger(__name__)

# Constantes
OUTPUT_FILENAME = f"offers_enriched_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
MINIO_OUTPUT_BUCKET = "traitement"
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "10"))  # Configurable via env var
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))
DELAY_BETWEEN_BATCHES = float(os.getenv("DELAY_BETWEEN_BATCHES", "1.0"))

def filter_data_profiles(profiles: list[dict]) -> list[dict]:
    """
    Garde uniquement les profils li√©s aux m√©tiers de la data (is_data_profile == True).
    """
    if not profiles:
        return []
    
    filtered = [p for p in profiles if p.get("is_data_profile") is True]
    logger.info(f"üîç Filtrage profils 'data' : {len(filtered)} sur {len(profiles)} conserv√©s")
    return filtered

def save_local_json(data: list[dict], filename: str):
    """
    Sauvegarde le JSON localement avec indentation lisible.
    """
    try:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        logger.info(f"üìÅ Donn√©es sauvegard√©es localement dans : {filename}")
        
        # Afficher quelques statistiques
        if data:
            logger.info(f"üìä Statistiques du fichier {filename}:")
            logger.info(f"   - Nombre total d'enregistrements: {len(data)}")
            
            # Compter les diff√©rents types de contrats
            contracts = {}
            companies = set()
            sectors = set()
            
            for item in data:
                contract = item.get('contrat', 'Non sp√©cifi√©')
                contracts[contract] = contracts.get(contract, 0) + 1
                
                if item.get('compagnie'):
                    companies.add(item['compagnie'])
                
                if item.get('secteur'):
                    sectors.add(item['secteur'])
            
            logger.info(f"   - Types de contrats: {dict(contracts)}")
            logger.info(f"   - Nombre d'entreprises uniques: {len(companies)}")
            logger.info(f"   - Nombre de secteurs uniques: {len(sectors)}")
            
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la sauvegarde de {filename}: {e}")
        raise

def generate_statistics(profiles: list[dict]) -> dict:
    """G√©n√®re des statistiques d√©taill√©es sur les profils"""
    if not profiles:
        return {}
    
    stats = {
        "total_profiles": len(profiles),
        "data_profiles": len([p for p in profiles if p.get("is_data_profile")]),
        "companies": len(set(p.get("compagnie") for p in profiles if p.get("compagnie"))),
        "sectors": len(set(p.get("secteur") for p in profiles if p.get("secteur"))),
        "contract_types": {},
        "experience_levels": {},
        "education_levels": {},
        "top_skills": {},
        "profiles_with_skills": len([p for p in profiles if p.get("skills")])
    }
    
    # Analyse des contrats
    for profile in profiles:
        contract = profile.get("contrat") or "Non sp√©cifi√©"
        stats["contract_types"][contract] = stats["contract_types"].get(contract, 0) + 1
    
    # Analyse des niveaux d'exp√©rience
    for profile in profiles:
        exp = profile.get("niveau_experience") or "Non sp√©cifi√©"
        stats["experience_levels"][exp] = stats["experience_levels"].get(exp, 0) + 1
    
    # Analyse des niveaux d'√©tudes
    for profile in profiles:
        edu = profile.get("niveau_etudes") or "Non sp√©cifi√©"
        stats["education_levels"][edu] = stats["education_levels"].get(edu, 0) + 1
    
    # Top skills
    skill_count = {}
    for profile in profiles:
        for skill in profile.get("skills", []):
            skill_name = skill.get("nom", "").strip()
            if skill_name:
                skill_count[skill_name] = skill_count.get(skill_name, 0) + 1
    
    # Top 15 skills
    stats["top_skills"] = dict(sorted(skill_count.items(), key=lambda x: x[1], reverse=True)[:15])
    
    return stats

def log_statistics(stats: dict):
    """Affiche les statistiques dans les logs"""
    if not stats:
        return
    
    logger.info("üìà === STATISTIQUES D√âTAILL√âES ===")
    logger.info(f"üìä Total profils: {stats['total_profiles']}")
    logger.info(f"üéØ Profils 'data': {stats['data_profiles']} ({stats['data_profiles']/stats['total_profiles']*100:.1f}%)")
    logger.info(f"üè¢ Entreprises uniques: {stats['companies']}")
    logger.info(f"üè≠ Secteurs uniques: {stats['sectors']}")
    logger.info(f"üéì Profils avec skills: {stats['profiles_with_skills']} ({stats['profiles_with_skills']/stats['total_profiles']*100:.1f}%)")
    
    if stats['contract_types']:
        logger.info("üìã Types de contrats:")
        for contract, count in sorted(stats['contract_types'].items(), key=lambda x: x[1], reverse=True):
            logger.info(f"   - {contract}: {count}")
    
    if stats['top_skills']:
        logger.info("üîß Top 10 comp√©tences:")
        for skill, count in list(stats['top_skills'].items())[:10]:
            logger.info(f"   - {skill}: {count}")

def validate_batch_results(batch_results: list, expected_count: int) -> list:
    """Valide les r√©sultats d'un batch et filtre les profils invalides"""
    valid_results = []
    
    for profile in batch_results:
        if not profile or not isinstance(profile, dict):
            continue
        
        # V√©rifier qu'il y a au moins quelques champs remplis
        filled_fields = sum(1 for v in profile.values() if v is not None and v != "" and v != [])
        
        if filled_fields >= 2:  # Au minimum 2 champs remplis
            valid_results.append(profile)
        else:
            logger.debug(f"Profil rejet√© (insuffisamment rempli): {profile}")
    
    # Si on a beaucoup moins de r√©sultats que pr√©vu, c'est suspect
    if len(valid_results) < expected_count * 0.5:
        logger.warning(f"‚ö†Ô∏è Nombre de profils valides ({len(valid_results)}) tr√®s inf√©rieur √† attendu ({expected_count})")
    
    return valid_results

def main():
    """Fonction principale du pipeline d'enrichissement Groq"""
    start_time = time.time()
    logger.info("üöÄ === D√âBUT DU PIPELINE D'ENRICHISSEMENT GROQ ===")
    logger.info(f"‚öôÔ∏è Configuration: BATCH_SIZE={BATCH_SIZE}, MAX_RETRIES={MAX_RETRIES}")

    # Test de connexion Groq
    if not test_groq_connection():
        logger.error("‚ùå Impossible de se connecter √† Groq, arr√™t du pipeline")
        return False

    try:
        # √âtape 1 : Chargement + normalisation
        logger.info("üì• √âtape 1: Chargement des offres depuis MinIO")
        offers = read_and_normalize_all_offers()
        
        if not offers:
            logger.warning("‚ùå Aucune offre charg√©e depuis MinIO, arr√™t.")
            return False

        logger.info(f"üìä {len(offers)} offres charg√©es depuis MinIO")
        all_profiles = []
        failed_batches = []

        # √âtape 2 : Traitement batch√© avec Groq
        logger.info("üß† √âtape 2: Traitement des offres par Groq")
        total_batches = (len(offers) + BATCH_SIZE - 1) // BATCH_SIZE
        
        for i in range(0, len(offers), BATCH_SIZE):
            batch = offers[i : i + BATCH_SIZE]
            batch_num = i // BATCH_SIZE + 1
            
            try:
                logger.info(f"üîÑ Traitement batch {batch_num}/{total_batches} ({len(batch)} offres)")
                
                # Appel Groq avec gestion d'erreur
                enriched_batch = call_groq(batch, retries=MAX_RETRIES)
                
                if not enriched_batch:
                    logger.warning(f"‚ö†Ô∏è Batch {batch_num} a retourn√© aucun r√©sultat")
                    failed_batches.append(batch_num)
                    continue
                
                # Validation des r√©sultats du batch
                valid_profiles = validate_batch_results(enriched_batch, len(batch))
                
                if not valid_profiles:
                    logger.warning(f"‚ö†Ô∏è Batch {batch_num}: aucun profil valide apr√®s validation")
                    failed_batches.append(batch_num)
                    continue
                
                # Validation et nettoyage final
                final_profiles = []
                for profile in valid_profiles:
                    if profile and any(profile.values()):  # V√©rifier que le profil n'est pas vide
                        validated_profile = validate_profile_fields(profile)
                        final_profiles.append(validated_profile)
                
                all_profiles.extend(final_profiles)
                logger.info(f"‚úÖ Batch {batch_num} trait√©: {len(final_profiles)} profils enrichis")
                
                # D√©lai entre les batches pour √©viter le rate limiting
                if batch_num < total_batches:
                    time.sleep(DELAY_BETWEEN_BATCHES)
                
            except Exception as e:
                logger.error(f"‚ùå Erreur lors du traitement du batch {batch_num}: {e}", exc_info=True)
                failed_batches.append(batch_num)
                continue

        # Rapport sur les √©checs
        if failed_batches:
            logger.warning(f"‚ö†Ô∏è {len(failed_batches)} batches ont √©chou√©: {failed_batches}")

        if not all_profiles:
            logger.warning("‚ö†Ô∏è Aucun profil enrichi r√©cup√©r√©, arr√™t.")
            return False

        logger.info(f"üìà Total des profils enrichis avant d√©duplication: {len(all_profiles)}")

        # √âtape 3 : D√©duplication globale
        logger.info("üßπ √âtape 3: D√©duplication des profils")
        all_profiles = deduplicate_profiles(all_profiles)
        logger.info(f"üìà Total des profils uniques: {len(all_profiles)}")

        # √âtape 4 : Filtrage profils m√©tiers data
        logger.info("üîç √âtape 4: Filtrage des profils 'data'")
        data_profiles = filter_data_profiles(all_profiles)
        
        if not data_profiles:
            logger.warning("‚ö†Ô∏è Aucun profil 'data' d√©tect√© apr√®s filtrage")
            logger.info("üíæ Sauvegarde de tous les profils enrichis...")
            
            # Sauvegarder tous les profils quand m√™me
            all_filename = f"all_{OUTPUT_FILENAME}"
            save_local_json(all_profiles, all_filename)
            
            try:
                save_to_minio(file_path=all_filename, bucket_name=MINIO_OUTPUT_BUCKET)
                logger.info(f"üì§ Tous les profils upload√©s sur MinIO: {all_filename}")
            except Exception as e:
                logger.error(f"‚ùå Erreur upload MinIO (tous profils): {e}")
            
            return True

        # √âtape 5 : G√©n√©ration des statistiques
        logger.info("üìä √âtape 5: G√©n√©ration des statistiques")
        stats = generate_statistics(data_profiles)
        log_statistics(stats)

        # √âtape 6 : Sauvegarde locale
        logger.info("üíæ √âtape 6: Sauvegarde locale")
        save_local_json(data_profiles, OUTPUT_FILENAME)

        # √âtape 7 : Upload sur MinIO
        logger.info("‚òÅÔ∏è √âtape 7: Upload sur MinIO")
        try:
            save_to_minio(file_path=OUTPUT_FILENAME, bucket_name=MINIO_OUTPUT_BUCKET)
            logger.info(f"üì§ Fichier upload√© sur MinIO bucket '{MINIO_OUTPUT_BUCKET}': {OUTPUT_FILENAME}")
            
            # Temps total d'ex√©cution
            total_time = time.time() - start_time
            logger.info(f"‚è±Ô∏è Temps total d'ex√©cution: {total_time:.2f} secondes")
            logger.info(f"üéâ === PIPELINE TERMIN√â AVEC SUCC√àS! ===")
            logger.info(f"üéØ R√©sum√©: {len(data_profiles)} profils 'data' trait√©s sur {len(offers)} offres initiales")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'upload MinIO: {e}")
            logger.info("üíæ Les donn√©es restent disponibles localement")
            return False

    except Exception as e:
        logger.error(f"‚ùå Erreur critique dans le pipeline: {e}", exc_info=True)
        return False

def main_with_recovery():
    """Version du main avec r√©cup√©ration automatique en cas d'erreur"""
    max_attempts = 3
    
    for attempt in range(1, max_attempts + 1):
        logger.info(f"üîÑ Tentative {attempt}/{max_attempts} du pipeline")
        
        try:
            success = main()
            if success:
                logger.info(f"‚úÖ Pipeline r√©ussi √† la tentative {attempt}")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è Pipeline √©chou√© √† la tentative {attempt}")
                if attempt < max_attempts:
                    wait_time = attempt * 30  # Attente progressive
                    logger.info(f"‚è≥ Attente de {wait_time}s avant nouvelle tentative...")
                    time.sleep(wait_time)
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur critique √† la tentative {attempt}: {e}")
            if attempt < max_attempts:
                wait_time = attempt * 60  # Attente plus longue en cas d'erreur
                logger.info(f"‚è≥ Attente de {wait_time}s avant nouvelle tentative...")
                time.sleep(wait_time)
    
    logger.error(f"‚õî Pipeline √©chou√© apr√®s {max_attempts} tentatives")
    return False

def health_check():
    """V√©rifie l'√©tat de sant√© des composants n√©cessaires"""
    logger.info("üè• V√©rification de l'√©tat de sant√© du syst√®me")
    
    checks = {
        "groq_connection": False,
        "minio_connection": False,
        "env_variables": False
    }
    
    # Test connexion Groq
    try:
        checks["groq_connection"] = test_groq_connection()
    except Exception as e:
        logger.error(f"‚ùå Test Groq √©chou√©: {e}")
    
    # Test variables d'environnement
    required_env_vars = ["GROQ_API_KEY"]
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    
    if not missing_vars:
        checks["env_variables"] = True
        logger.info("‚úÖ Variables d'environnement OK")
    else:
        logger.error(f"‚ùå Variables manquantes: {missing_vars}")
    
    # Test MinIO (indirectement via read_and_normalize_all_offers)
    try:
        test_offers = read_and_normalize_all_offers()
        if test_offers is not None:  # M√™me si vide, la fonction fonctionne
            checks["minio_connection"] = True
            logger.info("‚úÖ Connexion MinIO OK")
    except Exception as e:
        logger.error(f"‚ùå Test MinIO √©chou√©: {e}")
    
    # R√©sum√©
    all_checks_pass = all(checks.values())
    if all_checks_pass:
        logger.info("‚úÖ Tous les tests de sant√© passent")
    else:
        failed_checks = [check for check, status in checks.items() if not status]
        logger.error(f"‚ùå Tests √©chou√©s: {failed_checks}")
    
    return all_checks_pass

def run_pipeline_with_monitoring():
    """Lance le pipeline avec monitoring complet"""
    logger.info("üöÄ === D√âMARRAGE DU PIPELINE AVEC MONITORING ===")
    
    # Health check initial
    if not health_check():
        logger.error("‚ùå Health check √©chou√©, arr√™t du pipeline")
        return False
    
    # Informations syst√®me
    logger.info(f"üñ•Ô∏è Informations syst√®me:")
    logger.info(f"   - Heure de d√©marrage: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"   - BATCH_SIZE: {BATCH_SIZE}")
    logger.info(f"   - MAX_RETRIES: {MAX_RETRIES}")
    logger.info(f"   - DELAY_BETWEEN_BATCHES: {DELAY_BETWEEN_BATCHES}s")
    
    try:
        # Lancement du pipeline avec r√©cup√©ration
        success = main_with_recovery()
        
        if success:
            logger.info("üéâ === PIPELINE TERMIN√â AVEC SUCC√àS ===")
            return True
        else:
            logger.error("‚ùå === PIPELINE √âCHOU√â ===")
            return False
            
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è Pipeline interrompu par l'utilisateur")
        return False
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    # Configuration des arguments en ligne de commande (optionnel)
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "health":
            # Mode health check uniquement
            health_check()
        elif command == "test":
            # Mode test avec un petit √©chantillon
            original_batch_size = BATCH_SIZE
            BATCH_SIZE = 2  # Petits batches pour test
            logger.info("üß™ Mode test activ√© (BATCH_SIZE=2)")
            run_pipeline_with_monitoring()
        elif command == "recovery":
            # Mode avec r√©cup√©ration forc√©e
            main_with_recovery()
        else:
            logger.warning(f"‚ö†Ô∏è Commande inconnue: {command}")
            logger.info("Commandes disponibles: health, test, recovery")
            run_pipeline_with_monitoring()
    else:
        # Mode normal
        run_pipeline_with_monitoring()